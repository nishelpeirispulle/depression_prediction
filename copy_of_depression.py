# -*- coding: utf-8 -*-
"""Copy of depression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MpAYLJmkczbw1XaOBu8U3Lq-ii-bpWYf
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn import svm
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import RandomizedSearchCV
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.model_selection  import GridSearchCV

depression_dataset = pd.read_csv('/content/b_depressed.csv')

depression_dataset.head()

depression_dataset.shape

"""checking for null values"""

depression_dataset.isnull().values.any()

"""checking for null values inneach row"""

depression_dataset.isnull().sum()

"""checking skewness

"""

fig , ax = plt.subplots( figsize =(8,8))
sns.distplot(depression_dataset.no_lasting_investmen)

fig , ax = plt.subplots( figsize =(8,8))
sns.boxplot(depression_dataset.no_lasting_investmen)

"""Dropping the rows which contains null values"""

depression_dataset= depression_dataset.dropna(how='any')

"""checking for new no of rows and columns"""

depression_dataset.shape

depression_dataset

depression_datasetPairplot = depression_dataset.drop(['Survey_id' , 'Married' , 'Ville_id' , 'sex'  , 'gained_asset' , 'durable_asset' , 'save_asset' , 'living_expenses' , 'other_expenses' , 'incoming_salary' , 'incoming_own_farm' , 'incoming_business' , 'incoming_no_business' , 'incoming_agricultural' , 'farm_expenses' , 'labor_primary' , 'lasting_investment' , 'no_lasting_investmen'], axis=1)
depression_datasetPairplot.head()
plt.figure(figsize=(25,6))
sns.pairplot(data=depression_datasetPairplot,hue='depressed',plot_kws={'alpha':0.2})
plt.show()

depression_datasetPairplot = depression_dataset.drop(['save_asset','Survey_id' , 'Ville_id' , 'sex' , 'Age' , 'Married' , 'Number_children' , 'education_level' , 'total_members' , 'living_expenses' , 'other_expenses' , 'incoming_salary' , 'incoming_own_farm' , 'incoming_business' , 'incoming_no_business' , 'incoming_agricultural' , 'farm_expenses' , 'labor_primary' , 'lasting_investment' , 'no_lasting_investmen'], axis=1)
depression_datasetPairplot.head()
plt.figure(figsize=(25,6))
sns.pairplot(data=depression_datasetPairplot,hue='depressed',plot_kws={'alpha':0.2})
plt.show()

depression_datasetPairplot = depression_dataset.drop(['Survey_id' , 'Ville_id' , 'sex' , 'Age' , 'Married' , 'Number_children' , 'education_level' , 'total_members' , 'gained_asset' , 'durable_asset' , 'save_asset' , 'living_expenses' , 'other_expenses' , 'incoming_salary' , 'incoming_own_farm' , 'incoming_business' , 'incoming_no_business'  , 'labor_primary'     , 'no_lasting_investmen'], axis=1)
depression_datasetPairplot.head()
plt.figure(figsize=(25,6))
sns.pairplot(data=depression_datasetPairplot,hue='depressed',plot_kws={'alpha':0.2})
plt.show()

depression_datasetCorr = depression_dataset.drop(['no_lasting_investmen'], axis=1)

plt.subplots(figsize=(20,10)) 
sns.heatmap(depression_datasetCorr.corr(), annot = True, fmt = ".2f")
plt.show()

"""Dropping columns"""

depression_dataset =depression_dataset.drop(['no_lasting_investmen', 'Survey_id', 'Ville_id', 'gained_asset', 'durable_asset', 'save_asset', 'farm_expenses', 'labor_primary', 'Number_children','lasting_investment','incoming_agricultural'],axis=1)

"""updated dataset"""

depression_dataset

depression_dataset.describe()

"""Getting the count of depressed and non depressed people"""

depression_dataset['depressed'].value_counts()

depression_dataset.groupby('depressed').mean()

"""Seperating the features and labels"""

x= depression_dataset.drop(columns='depressed',axis=1)
y= depression_dataset['depressed']

print(x)

print(y)

"""#Standardization"""

scalar = StandardScaler()

"""fitting data"""

scalar.fit(x)

"""Transforming the data"""

Standardized_data = scalar.transform(x)

print(Standardized_data)

"""Updated features and previous label"""

x = Standardized_data
y = depression_dataset['depressed']

print(x)

print(y)

"""Splitting data into test and train"""

x_train,x_test,y_train,y_test = train_test_split(x , y , test_size=0.2  , random_state=2)

print(x.shape,x_train.shape, x_test.shape)

"""Handling class imbalance"""

from imblearn.over_sampling import RandomOverSampler
ros = RandomOverSampler(random_state=42)
x_res , y_res = ros.fit_resample(x_train, y_train)
y_res.shape
print('Before Over sampling',len([i for i in y_train if i==1]),len([i for i in y_train if i==0]))
print('After Over sampling',len([i for i in y_res if i==1]),len([i for i in y_res if i==0]))

x_train=x_res
y_train=y_res

"""Balanced dataset"""

depression_dataset









from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
from sklearn.metrics import cohen_kappa_score
from sklearn.metrics import roc_auc_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import roc_curve, auc

"""#Model creation

Decission Tree CLassifier
"""

from sklearn.tree import DecisionTreeClassifier

classifier_DT = DecisionTreeClassifier()

classifier_DT.fit(x_train, y_train)

x_train_prediction_DT = classifier_DT.predict(x_train)

training_data_accuracy_DT = accuracy_score(x_train_prediction_DT , y_train)

print('Accuracy score of the training data :',training_data_accuracy_DT)

x_test_prediction_DT = classifier_DT.predict(x_test)

test_data_accuracy_DT = accuracy_score(x_test_prediction_DT , y_test)

print('Accuracy score of the test data :',test_data_accuracy_DT)

def evaluate(model, x_test, y_test):
  pred = model.predict(x_test)
  accuracy = accuracy_score(y_test, pred)
  precision = precision_score(y_test, pred)
  recall = recall_score(y_test, pred)
  f1 = f1_score(y_test, pred)
  tn, fp, fn, tp = confusion_matrix(y_test, pred).ravel()
  print("TN: {}   FP: {}\nFN: {}   TP: {}".format(tn, fp, fn, tp))
  print(classification_report(y_test, pred))
  print('Accuracy: %f' % accuracy)
  print('Precision: %f' % precision)
  print('Recall: %f' % recall)
  print('F1 score: %f' % f1)

evaluate(classifier_DT , x_test , y_test)

"""Random forest classifier"""

classifier_RF = RandomForestClassifier()

classifier_RF.fit(x_train, y_train)

"""Model evaluation"""

x_train_prediction_RF = classifier_RF.predict(x_train)

"""Accuracy of training data"""

training_data_accuracy_RF = accuracy_score(x_train_prediction_RF , y_train)

print('Accuracy score of the training data :',training_data_accuracy_RF)

"""Accuracy of the test data"""

x_test_prediction_RF = classifier_RF.predict(x_test)

test_data_accuracy_RF = accuracy_score(x_test_prediction_RF , y_test)

print('Accuracy score of the test data :',test_data_accuracy_RF)



def evaluate(model, x_test, y_test):
  pred = model.predict(x_test)
  accuracy = accuracy_score(y_test, pred)
  precision = precision_score(y_test, pred)
  recall = recall_score(y_test, pred)
  f1 = f1_score(y_test, pred)
  tn, fp, fn, tp = confusion_matrix(y_test, pred).ravel()
  print("TN: {}   FP: {}\nFN: {}   TP: {}".format(tn, fp, fn, tp))
  print(classification_report(y_test, pred))
  print('Accuracy: %f' % accuracy)
  print('Precision: %f' % precision)
  print('Recall: %f' % recall)
  print('F1 score: %f' % f1)

"""Checking for precission and recall in the form of confusion matrix"""

evaluate(classifier_RF , x_test , y_test)

classifier_RF.get_params()



"""Xgboost classifier"""

import xgboost

classifier_xgboost = xgboost.XGBClassifier()

classifier_xgboost.fit(x_train, y_train)

x_train_prediction_xgboost = classifier_xgboost.predict(x_train)

training_data_accuracy_xgboost = accuracy_score(x_train_prediction_xgboost , y_train)

print('Accuracy score of the training data :',training_data_accuracy_xgboost)

x_test_prediction_xgboost = classifier_xgboost.predict(x_test)

test_data_accuracy_xgboost = accuracy_score(x_test_prediction_xgboost , y_test)

print('Accuracy score of the test data :',test_data_accuracy_xgboost)

def evaluate(model, x_test, y_test):
  pred = model.predict(x_test)
  accuracy = accuracy_score(y_test, pred)
  precision = precision_score(y_test, pred)
  recall = recall_score(y_test, pred)
  f1 = f1_score(y_test, pred)
  tn, fp, fn, tp = confusion_matrix(y_test, pred).ravel()
  print("TN: {}   FP: {}\nFN: {}   TP: {}".format(tn, fp, fn, tp))
  print(classification_report(y_test, pred))
  print('Accuracy: %f' % accuracy)
  print('Precision: %f' % precision)
  print('Recall: %f' % recall)
  print('F1 score: %f' % f1)

evaluate(classifier_xgboost , x_test , y_test)







classifier_xgboost.get_params()



from sklearn.neighbors import KNeighborsClassifier

"""KNN classifier"""

classifier_knn = KNeighborsClassifier()

classifier_knn.fit(x_train,y_train)

x_train_prediction_KNN = classifier_knn.predict(x_train)

training_data_accuracy_classifier = accuracy_score(x_train_prediction_KNN, y_train)

print('Accuracy score of the training data :', training_data_accuracy_classifier)

x_test_prediction_KNN = classifier_knn.predict(x_test)

test_data_accuracy = accuracy_score(x_test_prediction_KNN, y_test)

print('Accuracy score of the testing data:',test_data_accuracy)

def evaluate(model, x_test, y_test):
  pred = model.predict(x_test)
  accuracy = accuracy_score(y_test, pred)
  precision = precision_score(y_test, pred)
  recall = recall_score(y_test, pred)
  f1 = f1_score(y_test, pred)
  tn, fp, fn, tp = confusion_matrix(y_test, pred).ravel()
  print("TN: {}   FP: {}\nFN: {}   TP: {}".format(tn, fp, fn, tp))
  print(classification_report(y_test, pred))
  print('Accuracy: %f' % accuracy)
  print('Precision: %f' % precision)
  print('Recall: %f' % recall)
  print('F1 score: %f' % f1)

evaluate(classifier_knn , x_test , y_test)



"""support vector machine"""

classifier = svm.SVC( kernel = 'linear')

classifier.fit(x_train,y_train)

x_train_prediction_SVM = classifier.predict(x_train)

training_data_accuracy = accuracy_score (x_train_prediction_SVM, y_train)

print('Accuracy score of the training data : ', training_data_accuracy)

x_test_prediction_SVM = classifier.predict(x_test)

test_data_accuracy = accuracy_score(x_test_prediction_SVM , y_test)

print('Accuracy score of the testing data :' , test_data_accuracy)



def evaluate(model, x_test, y_test):
  pred = model.predict(x_test)
  accuracy = accuracy_score(y_test, pred)
  precision = precision_score(y_test, pred)
  recall = recall_score(y_test, pred)
  f1 = f1_score(y_test, pred)
  tn, fp, fn, tp = confusion_matrix(y_test, pred).ravel()
  print("TN: {}   FP: {}\nFN: {}   TP: {}".format(tn, fp, fn, tp))
  print(classification_report(y_test, pred))
  print('Accuracy: %f' % accuracy)
  print('Precision: %f' % precision)
  print('Recall: %f' % recall)
  print('F1 score: %f' % f1)

evaluate(classifier , x_train , y_train)

evaluate(classifier , x_test , y_test)

classifier.get_params()

"""Hyper parameter tuning"""

from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RepeatedStratifiedKFold

cv = RepeatedStratifiedKFold(n_splits=10,n_repeats=5 , random_state=1234)

"""param_grid = {'C': [0.1, 1, 10, 100,1000],
              'gamma': [ 0.01, 0.001, 0.0001],
              'kernel': ['rbf']}
"""

param_grid = { 'gamma': [ 0.1,0.01, 0.001, 0.0001],
              'degree':[1,2,3,4,5,6],
              'kernel': ['linear']
             }

grid = GridSearchCV(svm.SVC(), param_grid, refit = True, verbose = 3)

grid.fit(x_train, y_train)

model = grid.best_estimator_

grid.best_params_

grid.best_estimator_

grid.best_score_

evaluate(model,x_test,y_test)



DT_fpr, DT_tpr , threshold = roc_curve(y_test, x_test_prediction_DT)
auc_DT = auc(DT_fpr,DT_tpr)

RF_fpr, RF_tpr , threshold = roc_curve(y_test, x_test_prediction_RF)
auc_RF = auc(RF_fpr,RF_tpr)

KNN_fpr, KNN_tpr , threshold = roc_curve(y_test, x_test_prediction_KNN)
auc_KNN = auc(KNN_fpr,KNN_tpr)

SVM_fpr, SVM_tpr , threshold = roc_curve(y_test, x_test_prediction_SVM)
auc_SVM = auc(SVM_fpr,SVM_tpr)

plt.figure(figsize=(5,5),dpi=100)
plt.plot(DT_fpr, DT_tpr,marker='.', label='DT(auc=%0.3f)'% auc_DT)
plt.plot(RF_fpr, RF_tpr,marker='.', label='RF(auc=%0.3f)'% auc_RF)
plt.plot(KNN_fpr, KNN_tpr,marker='.', label='KNN(auc=%0.3f)'% auc_KNN)
plt.plot(SVM_fpr, SVM_tpr,marker='.', label='SVM(auc=%0.3f)'% auc_SVM)

plt.xlabel('False Positive Rate -->')
plt.ylabel('True Positive Rate -->')

plt.legend()


plt.show()





"""creating the Predictive system"""

input_data = (1,43,0,1,4,37369196,88084536,1,0,0,0)

"""changing input data into numpy array"""

input_data_as_numpy_array = np.asarray(input_data)

"""reshaping the array"""

input_data_reshaped = input_data_as_numpy_array.reshape(1,-1)

"""Standardize the input data"""

std_data=scalar.transform(input_data_reshaped)

print(std_data)

"""Making the prediction"""

prediction = classifier.predict(std_data)

print (prediction)

if (prediction[0]==0):
  print('The person is not depressed')
else:
  print('The person is depressed')

"""Saving the trained model"""

import pickle

filename='trained_model.save'

pickle.dump(classifier, open(filename, 'wb'))

"""loading the saved model"""

loaded_model = pickle.load(open('trained_model.save', 'rb'))

input_data = (1,43,0,1,4,37369196,88084536,1,0,0,0)

input_data_as_numpy_array = np.asarray(input_data)

input_data_reshaped = input_data_as_numpy_array.reshape(1,-1)

prediction = loaded_model.predict(std_data)

print(prediction)

if(prediction[0]==0):
  print('The person is not depressed')
else:
  print('The person is depressed')







